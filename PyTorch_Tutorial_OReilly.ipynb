{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "PyTorch_Tutorial_OReilly.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sLSxoMS198ZM",
        "xgmsSRw198ZR"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adantra/nangs/blob/master/PyTorch_Tutorial_OReilly.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fP31OuG98Xj"
      },
      "source": [
        "# Introduction to PyTorch - Neural Nets and Beyond\n",
        "\n",
        "## Robert Alvarez - Head of Data Science at Podium Education\n",
        "\n",
        "\n",
        "<!--![](data/img/odsc.png)-->\n",
        "\n",
        "<!---![](data/img/pydata-miami-logo.png)--->\n",
        "\n",
        "<!---![](data/img/GAIC.jpg)--->\n",
        "\n",
        "<!--![](data/img/Robert_Alvarez.jpg)-->\n",
        "\n",
        "![](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/Oreilly.jpg?raw=1)\n",
        "\n",
        "![](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/pytorch_logo.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8MMEUpx98Xk"
      },
      "source": [
        "---\n",
        "# Using PyTorch on Google Server\n",
        "\n",
        "Google has made a version of Jupyter Notebook available online for **free** that allow us to use GPUs for faster training time! I do not recommend you use the local installation unless you don't have access to the internet.\n",
        "\n",
        "Go to https://colab.research.google.com and sign in with your Google account. If you do not have a Google account you can create one. From there you can create a new notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmihBmzK-r04"
      },
      "source": [
        "# PyTorch on Google\n",
        "\n",
        "Google Colab has already pre-installed PyTorch (and many other libraries) so no need to re-install on our virtual machines.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-30T21:58:09.320174Z",
          "start_time": "2019-04-30T21:58:06.131991Z"
        },
        "id": "wM7YJvTf98Xl"
      },
      "source": [
        "# usual suspects\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import requests\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from cycler import cycler\n",
        "\n",
        "# the good stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "# standard sklearn import\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# minor changes to plotting functions\n",
        "import matplotlib.pyplot as plt\n",
        "cmap=plt.cm.tab10\n",
        "c = cycler('color', cmap(np.linspace(0,1,10)))\n",
        "plt.rcParams[\"axes.prop_cycle\"] = c\n",
        "\n",
        "\n",
        "# see if GPU is available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm78sdz_v0Qa",
        "outputId": "f226c57c-4695-4359-d519-a18c22865b61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Output should be: device(type='cuda', index=0)\n",
        "# If not, restart runtime with GPU enabled.\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XgbYeYa98Xn"
      },
      "source": [
        "# Download all necessary files for the live tutorial\n",
        "\n",
        "data_url = 'https://www.dropbox.com/s/k8ywqfx2hvrh9ny/pytorch_data.zip?dl=1'\n",
        "\n",
        "if not os.path.exists('data'):\n",
        "    # Download the data zip file.\n",
        "    response = requests.get(data_url, stream=True)\n",
        "    zip_path = 'pytorch_data.zip'\n",
        "    with open(zip_path, 'wb') as f:\n",
        "        shutil.copyfileobj(response.raw, f)\n",
        "    # Unzip the file.\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        z.extractall()\n",
        "    # Clean up.\n",
        "    os.remove(zip_path)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9lpGce498Xp"
      },
      "source": [
        "# PyTorch\n",
        "\n",
        "What is PyTorch?\n",
        "\n",
        "* It is a replacement for NumPy to use GPUs\n",
        "* A deep learning platform built for flexibility and speed\n",
        "\n",
        "\n",
        "## Tensor Overview\n",
        "What are tensors?\n",
        "\n",
        "Tensors are similar to NumPy's `ndarrays`\n",
        "\n",
        "We normally think of tensors as a generalization of matrices. In fact, matrices are 2-D tensors!\n",
        "\n",
        "![](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/grumpy_cat_tensor.jpg?raw=1)\n",
        "\n",
        "Here is a great visualization of tensors from 1-D to 5-D\n",
        "\n",
        "![](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/tensor_viz.jpeg?raw=1)\n",
        "\n",
        "As mentioned before, since tensors are generalizations of matrices, we should be able to create them in similar ways. We can also expect *most* operations to stay the same. In particular, addition of tensors is the same as for matrices. Multiplication is a bit different, but we won't have to concern ourselves with that in this lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCw6QKH298Xq"
      },
      "source": [
        "### Tensor Types\n",
        "\n",
        "Torch defines eight CPU tensor types and eight GPU tensor types:\n",
        "\n",
        "| Data type                | dtype                             | CPU Tensor           | GPU Tensor                |\n",
        "|--------------------------|-----------------------------------|----------------------|---------------------------|\n",
        "| 32-bit floating point    | `torch.float32` or `torch.float`  | `torch.FloatTensor`  | `torch.cuda.FloatTensor`  |\n",
        "| 64-bit floating point    | `torch.float64` or `torch.double` | `torch.DoubleTensor` | `torch.cuda.DoubleTensor` |\n",
        "| 16-bit floating point    | `torch.float16` or `torch.half`   | `torch.HalfTensor`   | `torch.cuda.HalfTensor`   |\n",
        "| 8-bit integer (unsigned) | `torch.uint8`                     | `torch.ByteTensor`   | `torch.cuda.ByteTensor`   |\n",
        "| 8-bit integer (signed)   | `torch.int8`                      | `torch.CharTensor`   | `torch.cuda.CharTensor`   |\n",
        "| 16-bit integer (signed)  | `torch.int16` or `torch.short`    | `torch.ShortTensor`  | `torch.cuda.ShortTensor`  |\n",
        "| 32-bit integer (signed)  | `torch.int32` or `torch.int`      | `torch.IntTensor`    | `torch.cuda.IntTensor`    |\n",
        "| 64-bit integer (signed)  | `torch.int64` or `torch.long`     | `torch.LongTensor`   | `torch.cuda.LongTensor`   |\n",
        "\n",
        "**Note**: Tensor types need to match when doing calculations with them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b_Zh-XD98Xq"
      },
      "source": [
        "### Numpy ndarrays vs PyTorch Tensors\n",
        "\n",
        "Let's look at the differences between these two common methods of handling arrays. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDuKQwQo98Xr",
        "outputId": "905eee97-ea24-455f-90a0-296c27ac5971",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# In numpy, we create tensors (arrays) in the following way\n",
        "x1 = np.random.rand(5,3)\n",
        "print(f\"x1 =\\n {x1}\\n\")\n",
        "\n",
        "# Similar to numpy we can create random tensors\n",
        "t1 = torch.rand(5,3, dtype=torch.float64)\n",
        "print(f\"t1 =\\n {t1}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x1 =\n",
            " [[0.36428855 0.75848623 0.903394  ]\n",
            " [0.85056056 0.24412658 0.79969996]\n",
            " [0.70910962 0.93960355 0.59116273]\n",
            " [0.05128521 0.81572304 0.50121405]\n",
            " [0.29226053 0.06085338 0.13616522]]\n",
            "\n",
            "t1 =\n",
            " tensor([[0.7095, 0.1938, 0.0802],\n",
            "        [0.8178, 0.2317, 0.3546],\n",
            "        [0.1929, 0.8668, 0.9334],\n",
            "        [0.4871, 0.9713, 0.2432],\n",
            "        [0.1025, 0.3794, 0.2546]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYXbEFaJ98Xt",
        "outputId": "b27929c0-6457-4ecc-afa8-8dd6d773a3c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# check the type\n",
        "print(f\"t1 is a {type(t1)}\")\n",
        "print(f\"x1 is a {type(x1)}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t1 is a <class 'torch.Tensor'>\n",
            "x1 is a <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckve1bS898Xu",
        "outputId": "e52ed2a0-15f6-45a1-b72e-1aa04855ad29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f\"t1 is dtype {t1.dtype}\")\n",
        "print(f\"x1 is dtype {x1.dtype}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t1 is dtype torch.float64\n",
            "x1 is dtype float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC9nKptd98Xw",
        "outputId": "10513de2-20c0-4209-f914-57599cd60515",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# create a 3-D tensor in torch and numpy\n",
        "t2 = torch.rand(2, 3, 5)\n",
        "print(f\"t2 =\\n {t2}\\n\")\n",
        "\n",
        "x2 = np.random.rand(2, 3, 5)\n",
        "print(f\"x2 =\\n {x2}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t2 =\n",
            " tensor([[[0.5323, 0.6318, 0.5135, 0.0929, 0.4394],\n",
            "         [0.4445, 0.0699, 0.9364, 0.1121, 0.0529],\n",
            "         [0.8673, 0.5533, 0.3253, 0.2019, 0.2893]],\n",
            "\n",
            "        [[0.0663, 0.2072, 0.1639, 0.5624, 0.7129],\n",
            "         [0.4298, 0.5382, 0.7287, 0.3156, 0.7065],\n",
            "         [0.9612, 0.3171, 0.2431, 0.0947, 0.4579]]])\n",
            "\n",
            "x2 =\n",
            " [[[0.77973755 0.68253514 0.87204106 0.2806437  0.50281242]\n",
            "  [0.82424035 0.24797129 0.19511466 0.88519936 0.93983781]\n",
            "  [0.6941165  0.03602898 0.66304882 0.97002251 0.11378839]]\n",
            "\n",
            " [[0.40065672 0.62305119 0.23474554 0.38599277 0.37897692]\n",
            "  [0.32089711 0.15348386 0.44605607 0.02262307 0.32985818]\n",
            "  [0.90522124 0.89541539 0.30430262 0.02378347 0.32558332]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLLno7I498Xy",
        "outputId": "20c845f2-e13b-4a59-9342-12e250f05ce3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "long_tensor = torch.zeros(5,3, dtype=torch.long)\n",
        "long_tensor"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjJxk3x898X1",
        "outputId": "a347cb62-65de-4a3c-d69d-2abe4b3259e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "float_tensor = torch.zeros(5,3, dtype=torch.float64)\n",
        "float_tensor"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1zfR0H198X4",
        "outputId": "bf18788e-cf3e-4ea4-ba12-ae121d79a4aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "long_tensor + float_tensor"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJXNRaUC98X7",
        "outputId": "3c60fc1b-7e07-4df7-a08a-3b41d8aefdc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# we can also create explicit tensors\n",
        "x = torch.tensor([2., 3])\n",
        "x, x.type()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([2., 3.]), 'torch.FloatTensor')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x8zEP4S98X9",
        "outputId": "dcd3a4a0-888a-4b18-f936-80df69c7b31a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# this method creates a new tensor \"y\" that has the same properties (e.g. dtype) as the original tensor \"x\"\n",
        "y = x.new_ones(5,3)\n",
        "print(f'{y.type()}\\n {y}')\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.FloatTensor\n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ldBCZ-Q98X_",
        "outputId": "dc5f514a-bc3b-463d-ab48-9d5900a71922",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# there's a size method as well\n",
        "y.size()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVYmRi0m98YB",
        "outputId": "3af7e88b-1735-4967-9c45-f42a38594ac9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We can add tensors as well in the usual way you expect\n",
        "x = torch.rand(5,3)\n",
        "x + y"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.1729, 1.0422, 1.6577],\n",
              "        [1.2962, 1.2558, 1.6949],\n",
              "        [1.6445, 1.9252, 1.5933],\n",
              "        [1.5125, 1.9534, 1.0421],\n",
              "        [1.0137, 1.1666, 1.2695]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z56pRLLn98YD",
        "outputId": "319b03fb-2615-4e1d-e5f3-d56b1fa1400b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We can also add like this\n",
        "torch.add(x, y)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.1729, 1.0422, 1.6577],\n",
              "        [1.2962, 1.2558, 1.6949],\n",
              "        [1.6445, 1.9252, 1.5933],\n",
              "        [1.5125, 1.9534, 1.0421],\n",
              "        [1.0137, 1.1666, 1.2695]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0PQJ4bI98YE",
        "outputId": "664fc65c-acac-40dc-c107-5fcd5714f947",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# OR like this!\n",
        "# The `*_` method works just like `inplace=True` in pandas\n",
        "y.add_(x) "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.1729, 1.0422, 1.6577],\n",
              "        [1.2962, 1.2558, 1.6949],\n",
              "        [1.6445, 1.9252, 1.5933],\n",
              "        [1.5125, 1.9534, 1.0421],\n",
              "        [1.0137, 1.1666, 1.2695]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38U4RzTj98YG",
        "outputId": "7d7c8220-721c-4e97-bce4-738de2e51b08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# see!\n",
        "y"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.1729, 1.0422, 1.6577],\n",
              "        [1.2962, 1.2558, 1.6949],\n",
              "        [1.6445, 1.9252, 1.5933],\n",
              "        [1.5125, 1.9534, 1.0421],\n",
              "        [1.0137, 1.1666, 1.2695]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KD9jmwr98YI",
        "outputId": "78c79b41-7858-42fd-cafd-e2cc9b2a2498",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Indexing/Slicing works just like in numpy\n",
        "y[2, :]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.6445, 1.9252, 1.5933])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfs11ngt98YX",
        "outputId": "1fccc1bd-9a7b-416b-9ca0-b56f945a47b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We can reshape tensors as well though it's called \"view\" in pytorch\n",
        "# support for 'reshape' has been added\n",
        "x = torch.randn(4,4)\n",
        "y = x.view(16)\n",
        "z = x.view(-1, 8)\n",
        "w = x.reshape(2, -1)\n",
        "print(x.size(), y.size(), z.size(), w.size())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) torch.Size([2, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XatTg6M498Ya"
      },
      "source": [
        "More on torch operations can be found here: https://pytorch.org/docs/stable/torch.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUce2CSj98Yb",
        "outputId": "0d8c1858-d8e3-4a0c-e5ac-553e82215b84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Converting to/from PyTorch & NumPy is easy\n",
        "a = torch.ones(5)\n",
        "a"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H10V5GL098Ye",
        "outputId": "ff8feaac-a8ea-4794-f19f-871144dee3c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# convert to numpy\n",
        "b = a.numpy()\n",
        "b"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxEOtLfN98Yg",
        "outputId": "2eb8a380-00b7-47da-a364-778be2205551",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# what do we expect this result to be?\n",
        "a.add_(1)\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2., 2., 2., 2., 2.])\n",
            "[2. 2. 2. 2. 2.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqnuTdCJ98Yi",
        "outputId": "45be1cfe-8a79-4712-8ce8-fef4694a3c5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Convert from numpy to torch\n",
        "c = np.random.randn(4,5)\n",
        "print(c)\n",
        "d = torch.from_numpy(c)\n",
        "print(d)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.03286458 -0.9302631   1.34745638  0.21790445  0.10241897]\n",
            " [ 0.50953387  0.51311176  0.12576333 -0.22041026  0.82960163]\n",
            " [-0.07024728  0.42497607  0.66539311 -1.28825542 -0.20455323]\n",
            " [ 0.66880426 -0.35107802  1.14003465 -0.47007954  0.05306217]]\n",
            "tensor([[ 0.0329, -0.9303,  1.3475,  0.2179,  0.1024],\n",
            "        [ 0.5095,  0.5131,  0.1258, -0.2204,  0.8296],\n",
            "        [-0.0702,  0.4250,  0.6654, -1.2883, -0.2046],\n",
            "        [ 0.6688, -0.3511,  1.1400, -0.4701,  0.0531]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvnyYAJ698Yk"
      },
      "source": [
        "## Autograd - AKA why PyTorch is awesome\n",
        "\n",
        "Central to all Neural Networks in PyTorch is the `autograd` package. \n",
        "\n",
        "The `autograd` package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, \n",
        "which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n",
        "\n",
        "Let's look at a basic example of this before turning to Neural Networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqIAiQl798Yk",
        "outputId": "b3afa007-149e-4502-9976-a01606ca7e3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create a tensor and set requires_grad=True to track computation with it\n",
        "x = torch.ones(2,2, requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hCV7jWG98Ym",
        "outputId": "0021e29e-7d35-4689-b397-7c62b483d1ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Do some operation on said tensor\n",
        "y = 3*x + 7\n",
        "print(y)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[10., 10.],\n",
            "        [10., 10.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpzudC3v98Yn",
        "outputId": "e069cc4f-f1d7-49bd-91b7-f231bdc94610",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Because y was created as a result of an operation, it now has a grad_fn method\n",
        "y.grad_fn"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AddBackward0 at 0x7f697c401b10>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl0fduSM98Yo",
        "outputId": "33848ae0-798a-42c3-de1d-b4efe61f2464",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We can do more stuff to y (and thus x) and calculate its derivatives\n",
        "z = 2*y**2\n",
        "w = z.mean()\n",
        "\n",
        "print(z, w)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[200., 200.],\n",
            "        [200., 200.]], grad_fn=<MulBackward0>) tensor(200., grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fz1lXOm4LwQ"
      },
      "source": [
        "Let's do that derivative by hand and see that we get the same result.\n",
        "\n",
        "$\\frac{\\partial w}{\\partial x} = \\frac{\\partial w}{\\partial z} \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}$\n",
        "\n",
        "Which gives us \n",
        "\n",
        "$w(x) = \\frac{1}{4} z(x)^2 = \\frac{1}{2} (3x+7)^2$\n",
        "\n",
        "$w'(x) = (3x+7)*3|_{x=1} = 30$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-9pbJF698Yp"
      },
      "source": [
        "# Backpropagation in one line!\n",
        "w.backward()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrscC7cq98Yr",
        "outputId": "796b32de-a083-4e84-f305-153eef2f5aa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# et voila!\n",
        "x.grad"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[30., 30.],\n",
              "        [30., 30.]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZBQt0mX98Yt"
      },
      "source": [
        "## Linear Regression Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T00:53:01.316904Z",
          "start_time": "2019-05-02T00:53:01.309295Z"
        },
        "id": "KeTo0Pb_98Yu"
      },
      "source": [
        "n=1000"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T00:53:01.886074Z",
          "start_time": "2019-05-02T00:53:01.876908Z"
        },
        "id": "hJ4LRmVQ98Yv",
        "outputId": "2e0f115c-3b15-4ca7-b222-07bcf4ffa37a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = torch.ones(n,2) \n",
        "x[:,0].uniform_(-1.,1)\n",
        "x[:5]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.2012,  1.0000],\n",
              "        [ 0.2853,  1.0000],\n",
              "        [-0.2048,  1.0000],\n",
              "        [ 0.4733,  1.0000],\n",
              "        [ 0.9194,  1.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T00:55:25.633499Z",
          "start_time": "2019-05-02T00:55:25.617471Z"
        },
        "id": "Yrp1OMEe98Yx",
        "outputId": "1798e77b-b026-4bdc-dcfd-dbb01fc388dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a = torch.tensor([3.,2]); a"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3., 2.])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T00:55:26.210711Z",
          "start_time": "2019-05-02T00:55:26.197229Z"
        },
        "id": "P2k3oBNf98Yy"
      },
      "source": [
        "y = x@a + torch.randn(n)/3"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPFm3Yy798Y0",
        "outputId": "99dbbd50-4932-4b4a-a897-a8b46821c683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "plt.scatter(x[:,0], y);"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGjHe8Hz98Y2"
      },
      "source": [
        "def mse(y, y_pred): \n",
        "    return ((y - y_pred)**2).mean()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKgWWEro98Y3"
      },
      "source": [
        "a_guess = torch.tensor([-1.,1])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlVJnrog98Y5",
        "outputId": "ade20449-2349-4350-cd79-db8478e3f131",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_hat = x@a_guess # @ is matrix multiplication\n",
        "mse(y_hat, y)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.5470)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv6Qha9A98Y7",
        "outputId": "8f15debb-3dc0-4827-8cca-be2d4bf0b330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "plt.scatter(x[:,0],y)\n",
        "plt.scatter(x[:,0],y_hat);"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4ahptpE98Y8",
        "outputId": "255eff25-522e-4def-9588-8d689cefb106",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a_guess = nn.Parameter(a_guess)\n",
        "a_guess"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([-1.,  1.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75TK6zOR98Y9"
      },
      "source": [
        "def update():\n",
        "    '''\n",
        "    function to update tensor using SGD\n",
        "    '''\n",
        "    y_hat = x@a_guess\n",
        "    loss = mse(y, y_hat)\n",
        "    if t % 10 == 0: print(loss)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        a_guess.sub_(lr * a_guess.grad)\n",
        "        a_guess.grad.zero_()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce9_sd1v98Y-",
        "outputId": "e053095d-130d-43d7-b89d-a51f8e15929b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lr = 1e-1\n",
        "for t in range(100): \n",
        "    update()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(6.5470, grad_fn=<MeanBackward0>)\n",
            "tensor(1.4261, grad_fn=<MeanBackward0>)\n",
            "tensor(0.4557, grad_fn=<MeanBackward0>)\n",
            "tensor(0.2087, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1448, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1283, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1240, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1229, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1226, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1225, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeUXfjdV98ZB",
        "outputId": "50fe1484-1d4a-4db8-bce7-cf32ef5e0bd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "plt.scatter(x[:,0], y)\n",
        "plt.scatter(x[:,0], x@a_guess.detach()); # detach() removes gradient from `a` so it can be plotted. otherwise matplotlib gives error"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSjr7WCF98ZG"
      },
      "source": [
        "### Normal Equations\n",
        "Although we solved for the line of best fit using stochastic gradient descent,\n",
        "we could have also used the [Normal Equations](https://en.wikipedia.org/wiki/Linear_least_squares#Derivation_of_the_normal_equations) given by\n",
        "\n",
        "$$ \\theta = (X^T X)^{-1} X^T \\vec y$$\n",
        "\n",
        "where $\\theta$ is the vector we called `a` in the linear regression code above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T00:57:18.751491Z",
          "start_time": "2019-05-02T00:57:18.743154Z"
        },
        "id": "G9Qwa5o498ZG"
      },
      "source": [
        "# Exercise:\n",
        "# Use pytorch to solve for theta using the normal equations\n",
        "# plot your result when you are finished\n",
        "\n",
        "# HINT:\n",
        "# matrix inverse in pytorch: x.inverse()\n",
        "# matrix transpose in pytorch: torch.t(x)\n",
        "\n",
        "# Code goes here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhcElduD98ZH"
      },
      "source": [
        "---\n",
        "# Neural Nets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUSiq8Hs98ZI"
      },
      "source": [
        "Neural networks (NNs) are special forms of nonlinear regressions where the decision system for which the NN is built mimics the way \n",
        "the brain is supposed to work (whether it works like a NN is up for grabs, of course).\n",
        "\n",
        "Like many of the algorithms we have seen in classical machine learning, it is a supervised learning technique that can perform complex tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ814fgi98ZI"
      },
      "source": [
        "## Perceptrons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YQwp2rp98ZJ"
      },
      "source": [
        "The basic building block of a neural network is a perceptron. A perceptron is like a neuron in a human brain. It takes inputs \n",
        "(e.g. sensory in a real brain) and then produces an output signal. An entire network of perceptrons is called a neural net.\n",
        "\n",
        "![linear regression](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/NN_perceptron.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00sR2ipX98ZJ"
      },
      "source": [
        "In general, a perceptron could have more or fewer inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XldQsuBe98ZK"
      },
      "source": [
        "Instead of assigning equal weight to each of the inputs, we can assign real numbers $w_1, w_2, \\ldots$ expressing the importance of the respective inputs to the output. The nueron's output, 0 or 1, is determined whether the weighted sum $\\sum_j w_j x_j$ is less than or greater than some *threshold value*.\n",
        "\n",
        "Perceptrons may emit continuous signals or binary $(0,1)$ signals. In the case of a credit card application, the final perceptron is a binary one (approved or denied). Such perceptrons are implemented by means of squashing functions. For example, a really simple squashing function is one that issues a 1 if the function value is positive and a $-1$ if it is negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb28h7Ll98ZK"
      },
      "source": [
        "To put this in more mathematical terms, let $z = \\sum_{j=0}^n w_j x_j$ . \n",
        "Then the *activation function* $\\phi(z)$ is defined as \n",
        "\n",
        "$$\n",
        "\\phi(z) =\n",
        "\\begin{cases}\n",
        "-1 & \\text{if } z < \\theta\\\\\n",
        " 1 & \\text{if } z \\geq \\theta\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if9rqZt-98ZL"
      },
      "source": [
        "![](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/activation-function.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5JMM3rJ98ZL"
      },
      "source": [
        "The whole point of the perceptron is to mimic how a single nueron in the brain works: it either *fires* or it doesn't. Thus, the \n",
        "perceptron rule is fairly simple and can be summarized by the following steps.\n",
        "\n",
        "* Initialize the weights to zero or small random numbers\n",
        "* For each training sample $\\textbf{x}_n$ perform the following steps:\n",
        "    * Compute the output value $y$\n",
        "    * Calculate error in $y$ vs $\\hat y$\n",
        "    * Update the weights\n",
        "  \n",
        "Here, the output value is the class label predicted by the activation function that we defined earlier, and the\n",
        "simultaneous update of weight $w_j$ in the weight vector $\\textbf{w}$ can be more formally written as\n",
        "\n",
        "$$\\bar w_j = w_j + \\Delta w_j$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLSxoMS198ZM"
      },
      "source": [
        "## Fitting a model\n",
        "\n",
        "Let's go back to our linear perceptron. It has the following parameters:\n",
        "\n",
        "* $x_i$: inputs\n",
        "* $y$ : output\n",
        "* $w_i$: learned weights\n",
        "\n",
        "What we would like to do is adjust the $w_i$'s until our model has the best fit.\n",
        "\n",
        "First, initialize the $w_i$'s in some meaningful way (usually they're drawn from a randon uniform distribution).\n",
        "\n",
        "Then, we put it into the usual **algorithm workflow:**\n",
        "\n",
        "* calculate prediction $\\hat y$\n",
        "* calculate Loss function $L(y, \\hat y)$\n",
        "* update weights using backpropagation\n",
        "\n",
        "### Loss function and backpropagation\n",
        "\n",
        "To figure out how well our prediction was during each epoch, we'll use a basic loss function, mean squared error (MSE):\n",
        "\n",
        "$L(y,\\hat{y}) = ||~ y-\\hat{y} ~||^2$,\n",
        "\n",
        "ultimately trying to find $L_{\\rm min}$, defined by the point in parameter space where $\\nabla_{w_i} L = 0$.\n",
        "\n",
        "Per-iteration update: \n",
        "\n",
        "$ w_i \\to w_i - \\eta \\nabla_{w_i} L $,\n",
        "\n",
        "where $\\eta$ is known as the learning rate; too small and takes very long to converge, too big and you oscillate about the minimum.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd5YUrwu98ZM"
      },
      "source": [
        "## A basic example\n",
        "\n",
        "We'll build a Rube Goldberg adding machine that will illustrate how neural nets work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YHrn_ov98ZM"
      },
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 1, bias=False))\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.MSELoss()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJrYvWGK98ZO"
      },
      "source": [
        "total_loss = []\n",
        "num_samples = 10_000\n",
        "\n",
        "for num in range(1, num_samples+1):\n",
        "    # Progress bar indicator\n",
        "    if num % (num_samples//5) == 0:\n",
        "        print('{0}: %: {1:.3f}'.format(num,num/num_samples * 100))\n",
        "        \n",
        "    # data prep\n",
        "    x = 4*torch.rand(2) #generate two random numbers uniformly on (0,4)\n",
        "    data, target = Variable(x), Variable(x[0] + x[1])\n",
        "    \n",
        "    # Feed forward through NN\n",
        "    output = model(data)\n",
        "    loss = criterion(output, target)\n",
        "    total_loss.append(loss)\n",
        "    \n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOtHnPe598ZP"
      },
      "source": [
        "fig,ax=plt.subplots(figsize=(11,8))\n",
        "ax.plot(total_loss,marker='.',ls='',markersize=1.)\n",
        "ax.set_ylim(0,);ax.set_xlim(0,);ax.grid(alpha=0.2);\n",
        "ax.set_xlabel('training examples');ax.set_ylabel('mean squared loss');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPNZtx5598ZQ"
      },
      "source": [
        "x,y=np.linspace(-5,8,100),[]\n",
        "\n",
        "for xx in x:\n",
        "    yy=model(torch.tensor([xx,2], dtype=torch.float32)).data.cpu().numpy()[0]\n",
        "    y.append(yy)\n",
        "y=np.array(y)\n",
        "\n",
        "fig,ax=plt.subplots(figsize=(11,8))\n",
        "ax.plot([-5,8],[-3,10],lw=2.0,label='actual',alpha=0.7)\n",
        "ax.fill_betweenx([-3,10],0,4,alpha=0.2)\n",
        "ax.scatter(x,y,marker='.',s=3.,label='prediction',color='r')\n",
        "ax.text(0.2,0,'Where we have \\ntraining data')\n",
        "ax.legend()\n",
        "ax.set_ylim(-3,10);ax.set_xlim(-5,8);\n",
        "ax.grid(alpha=0.2);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgmsSRw198ZR"
      },
      "source": [
        "## Feedforward Neural Network\n",
        "\n",
        "![General Feed Forward Network](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/neural_net_123.png?raw=1)\n",
        "\n",
        "For our case of learning linear relationships, the modification to the linear regression architecture is depicted below:\n",
        "\n",
        "\n",
        "![non-linear activation](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/linear_regression_activation.png?raw=1)\n",
        "\n",
        "where\n",
        "\n",
        "$$\\varphi(z) = \\frac{1}{1+e^{-z}}$$\n",
        "\n",
        "is the so-called sigmoid function; this is typically the activation function that is first introduced, I think because of historical reasons. In modern practice, it finds most of its use in transforming single outputs from a NN into a probability. It's worth noting that if your NN will output multiple probabilities, for example, if your NN will categorize between black cats, red cats, white cats, etc., a multi-dimensional generalization of the sigmoid, called the softmax function, is typically used. \n",
        "\n",
        "The motivation behind adding an activation function is the hope that the NN model may capture non-linear relationships that exist in the data. Below are some commonly used activation functions. \n",
        "\n",
        "![activation functions](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/activation_function_list.png?raw=1)\n",
        "\n",
        "In practice, a lot of architectures use the rectified linear unit (ReLU), along with it's close cousin, the so-called leaky-ReLU. In introducing this idea though, we'll focus on the sigmoid which maps real numbers from $(-\\infty,\\infty) \\to [0,1]$.\n",
        "\n",
        "Of course our data is linear in the case of a straight line (!) but let's see what happens if we try to force a non-linear activation layer to capture a linear relationship.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcvWinIf98ZR"
      },
      "source": [
        "## Non-linear model for a linear relationship\n",
        "\n",
        "### Deep Feedforward Network with sigmoid activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T00:26:19.752876Z",
          "start_time": "2019-05-02T00:26:19.742303Z"
        },
        "id": "r4Qatq-r98ZS"
      },
      "source": [
        "model = nn.Sequential(\n",
        "            nn.Linear(2, 20),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(20, 20),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(20, 20),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(20, 1))\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.MSELoss()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T00:47:02.232283Z",
          "start_time": "2019-05-02T00:46:52.792621Z"
        },
        "id": "LJV3Gs4Z98ZT"
      },
      "source": [
        "total_loss = []\n",
        "num_samples = 10_000\n",
        "\n",
        "for num in range(1, num_samples+1):\n",
        "    # Progress bar indicator\n",
        "    if num % (num_samples//5) == 0:\n",
        "        print('{0}: %: {1:.3f}'.format(num,num/num_samples * 100))\n",
        "        \n",
        "    # data prep\n",
        "    x = 4*torch.rand(2) #generate two random numbers uniformly on (0,4)\n",
        "    data, target = Variable(x), Variable(x[0] + x[1])\n",
        "    \n",
        "    # Feed forward through NN\n",
        "    output = model(data)\n",
        "    loss = criterion(output, target)\n",
        "    total_loss.append(loss)\n",
        "    \n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T00:47:24.128293Z",
          "start_time": "2019-05-02T00:47:23.259386Z"
        },
        "id": "sg8t7Nju98ZU"
      },
      "source": [
        "fig,ax=plt.subplots(figsize=(11,8))\n",
        "ax.plot(total_loss,marker='.',ls='',markersize=.8)\n",
        "ax.set_ylim(0,);ax.set_xlim(0,);ax.grid(alpha=0.2);\n",
        "ax.set_xlabel('training examples');ax.set_ylabel('mean squared loss');\n",
        "\n",
        "x,y=np.linspace(-5,8,100),[]\n",
        "\n",
        "for xx in x:\n",
        "    yy=model(torch.tensor([xx,2],dtype=torch.float32)).data.cpu().numpy()[0]\n",
        "    y.append(yy)\n",
        "y=np.array(y)\n",
        "\n",
        "fig,ax=plt.subplots(figsize=(11,8))\n",
        "ax.plot([-5,8],[-3,10],lw=2.0,label='actual',alpha=0.7)\n",
        "ax.fill_betweenx([-3,10],0,4,alpha=0.2)\n",
        "ax.scatter(x,y,marker='.',s=3.,label='prediction',color='r')\n",
        "ax.text(0.2,0,'Where we have \\ntraining data')\n",
        "ax.legend()\n",
        "ax.set_ylim(-3,10);ax.set_xlim(-5,8);\n",
        "ax.grid(alpha=0.2);\n",
        "ax.set_title('Sigmoid activation function');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MF1JD0898ZV"
      },
      "source": [
        "## ReLU activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkrdWaTZ98ZX"
      },
      "source": [
        "model = nn.Sequential(\n",
        "            nn.Linear(2, 20),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(20, 20),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(20, 20),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(20, 1))\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.MSELoss()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81jF1PRV98ZY"
      },
      "source": [
        "total_loss = []\n",
        "num_samples = 10_000\n",
        "\n",
        "for num in range(1, num_samples+1):\n",
        "    # Progress bar indicator\n",
        "    if num % (num_samples//5) == 0:\n",
        "        print('{0}: %: {1:.3f}'.format(num,num/num_samples * 100))\n",
        "        \n",
        "    # data prep\n",
        "    x = 4*torch.rand(2) #generate two random numbers uniformly on (0,4)\n",
        "    data, target = Variable(x), Variable(x[0] + x[1])\n",
        "    \n",
        "    # Feed forward through NN\n",
        "    output = model(data)\n",
        "    loss = criterion(output, target)\n",
        "    total_loss.append(loss)\n",
        "    \n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8lVbhVS98ZZ"
      },
      "source": [
        "fig,ax=plt.subplots(figsize=(11,8))\n",
        "ax.plot(total_loss,marker='.',ls='',markersize=.8)\n",
        "ax.set_ylim(0,);ax.set_xlim(0,);ax.grid(alpha=0.2);\n",
        "ax.set_xlabel('training examples');ax.set_ylabel('mean squared loss');\n",
        "\n",
        "x,y=np.linspace(-5,8,100),[]\n",
        "\n",
        "for xx in x:\n",
        "    yy=model(torch.tensor([xx,2],dtype=torch.float32)).data.cpu().numpy()[0]\n",
        "    y.append(yy)\n",
        "y=np.array(y)\n",
        "\n",
        "fig,ax=plt.subplots(figsize=(11,8))\n",
        "ax.plot([-5,8],[-3,10],lw=2.0,label='actual',alpha=0.7)\n",
        "ax.fill_betweenx([-3,10],0,4,alpha=0.2)\n",
        "ax.scatter(x,y,marker='.',s=3.,label='prediction',color='r')\n",
        "ax.text(0.2,0,'Where we have \\ntraining data')\n",
        "ax.legend()\n",
        "ax.set_ylim(-3,10);ax.set_xlim(-5,8);\n",
        "ax.grid(alpha=0.2);\n",
        "ax.set_title('ReLu activation function');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge84bd3z98Za"
      },
      "source": [
        "## Teaching a machine to draw circles\n",
        "\n",
        "Here the NN learns attempts to learn the 2d rotation matrix, parameterized by the generator of rotations in two dimensions:\n",
        "\n",
        "$R={\\begin{bmatrix}\\cos \\theta &-\\sin \\theta \\\\\\sin \\theta &\\cos \\theta \\\\\\end{bmatrix}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-30T05:42:28.228388Z",
          "start_time": "2019-04-30T05:42:28.187133Z"
        },
        "id": "D-LHtrI998Zb"
      },
      "source": [
        "# First some helper functions for plotting and model training\n",
        "def train_models(models, optimizers, num_samples=1000, circle_interval=1.0, save_models=False, cuda=torch.cuda.is_available(), recurrent=False):\n",
        "    total_loss = []\n",
        "    for num in range(1, num_samples+1):\n",
        "        # progress indicator \n",
        "        if num % (num_samples//20) ==0:\n",
        "            print('{0}: %: {1:.3f}'.format(num, num/num_samples * 100))\n",
        "            \n",
        "        # data calc \n",
        "        # take a random point on the circle of radius 1\n",
        "        x, theta = torch.ones(2), circle_interval*2*np.pi*torch.rand(1)\n",
        "        R = torch.zeros(2,2)\n",
        "        R[0,:] = torch.Tensor([np.cos(theta[0]),-np.sin(theta[0])])\n",
        "        R[1,:] = torch.Tensor([np.sin(theta[0]), np.cos(theta[0])])\n",
        "        \n",
        "        data, target = Variable(theta), Variable(torch.mv(R,x))\n",
        "        \n",
        "        # Check if GPU can be used\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            \n",
        "        # learning phases \n",
        "        for idx, model in enumerate(models):\n",
        "            loss_iter = []\n",
        "            # forward \n",
        "            if recurrent:\n",
        "                output = model(data,None)\n",
        "            else:\n",
        "                output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss_iter.append(loss.data.item())\n",
        "            # backward \n",
        "            optimizers[idx].zero_grad()\n",
        "            loss.backward()\n",
        "            optimizers[idx].step()\n",
        "        total_loss.append(np.mean(loss_iter))\n",
        "        \n",
        "    # save model state \n",
        "    if save_models:\n",
        "        for l,model in enumerate(models):\n",
        "            torch.save(model.state_dict(), 'rotations_{}.pth'.format(l))\n",
        "    return total_loss,theta\n",
        "        \n",
        "def plot_circles(models, offset=0, CI=False):\n",
        "    fig, axes = plt.subplots(figsize=(5*3,3.9),ncols=3)\n",
        "    x = torch.ones(2)\n",
        "    for k,ax in enumerate(axes):\n",
        "        ax.scatter(x[0],x[1], facecolors='none', edgecolors='r')\n",
        "        ax.scatter(x[0],x[1], facecolors='none', edgecolors='b')\n",
        "        x_real, y_real = [],[]\n",
        "        x_mean, y_mean = [],[]\n",
        "        x_std, y_std = [],[]\n",
        "        for theta in np.linspace((k+offset) *2*np.pi,(k+1+offset) *2*np.pi,300):\n",
        "            x_model,y_model = [],[]\n",
        "            # synthetic (real) data \n",
        "            data = Variable(torch.Tensor([theta]))#.cuda()\n",
        "            R = torch.zeros(2,2)\n",
        "            R[0,:] = torch.Tensor([np.cos(theta),-np.sin(theta)])\n",
        "            R[1,:] = torch.Tensor([np.sin(theta), np.cos(theta)])\n",
        "            real = torch.mv(R,x)\n",
        "            x_real.append(real[0].numpy())\n",
        "            y_real.append(real[1].numpy())\n",
        "            # predict w/ all models \n",
        "            for model in models:\n",
        "                if torch.cuda.is_available():\n",
        "                    model.cpu()\n",
        "                    outputs=model(data).data\n",
        "                    xx_model, yy_model = outputs[0],outputs[1]\n",
        "                    x_model.append(xx_model.numpy())\n",
        "                    y_model.append(yy_model.numpy())\n",
        "                else:\n",
        "                    outputs=model(data).data\n",
        "                    xx_model, yy_model = outputs[0],outputs[1]\n",
        "                    x_model.append(xx_model.numpy())\n",
        "                    y_model.append(yy_model.numpy())\n",
        "            # summarize all model predictions \n",
        "            x_mean.append(np.mean(x_model))\n",
        "            y_mean.append(np.mean(y_model))\n",
        "            x_std.append(np.std(x_model))\n",
        "            y_std.append(np.std(y_model))\n",
        "        # plotting data \n",
        "        ax.scatter(x_real,y_real, facecolors='none', edgecolors='r',label='real data',s=2.)\n",
        "        ax.scatter(x_mean,y_mean, facecolors='none', edgecolors='k',label='model data', alpha=0.9,s=2.)\n",
        "        if CI:\n",
        "            ax.fill_betweenx(y_mean,x_mean-3*np.array(x_std),x_mean+3*np.array(x_std), alpha=0.1,color='b')\n",
        "            ax.fill_between(x_mean,y_mean-3*np.array(y_std),y_mean+3*np.array(y_std), alpha=0.1,color='b')\n",
        "        ax.legend()\n",
        "        ax.set_ylim(-2,2);ax.set_xlim(-2,2);ax.grid(alpha=0.3)\n",
        "        ax.set_title(r'${}\\pi \\leq \\theta \\leq {}\\pi$'.format(2*(k+offset),2*(k+1+offset)),y=1.01);\n",
        "    \n",
        "    return x_mean, y_mean, np.array(x_std), np.array(y_std)\n",
        "        \n",
        "def weight_init(m): # so-called xavier normalization https://arxiv.org/abs/1211.5063\n",
        "    if isinstance(m, nn.Linear):\n",
        "        size = m.weight.size()\n",
        "        fan_out = size[0]\n",
        "        fan_in = size[1]\n",
        "        variance = np.sqrt(2.0/(fan_in + fan_out))\n",
        "        m.weight.data.normal_(0.0, variance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-30T05:47:50.984630Z",
          "start_time": "2019-04-30T05:45:17.951543Z"
        },
        "id": "aLPdSOBI98Zd"
      },
      "source": [
        "num_nodes=10\n",
        "\n",
        "# Usually we define neural nets as a class in pytorch\n",
        "class Rotations(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Rotations, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(1,num_nodes),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(num_nodes,2))      \n",
        "    def forward(self, x):\n",
        "        out=self.layer1(x)\n",
        "        return out\n",
        "\n",
        "model = Rotations().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.MSELoss()\n",
        "print(model)\n",
        "\n",
        "total_loss, theta = train_models([model], [optimizer], num_samples=100000, circle_interval=2.0)\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(11,8))\n",
        "ax.plot(total_loss,marker='.',ls='',markersize=.8)\n",
        "ax.set_ylim(0,);ax.set_xlim(0,);ax.grid(alpha=0.2);\n",
        "ax.set_xlabel('training examples');ax.set_ylabel('mean squared loss');\n",
        "\n",
        "output=plot_circles([model],offset=0,CI=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXBgBf1O98Zg"
      },
      "source": [
        "### Will a deeper network perform better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-30T06:05:29.936212Z",
          "start_time": "2019-04-30T06:00:59.778558Z"
        },
        "id": "Gk0GC2Ry98Zg"
      },
      "source": [
        "# Add more layers to the model and see if performance on the test set increases\n",
        "num_nodes=10\n",
        "class Rotations(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Rotations, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(1,num_nodes),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(num_nodes,num_nodes),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(num_nodes,num_nodes),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(num_nodes,2))      \n",
        "    def forward(self, x):\n",
        "        out=self.layer1(x)\n",
        "        return out\n",
        "\n",
        "model=Rotations().to(device)\n",
        "optimizer=torch.optim.Adam(model.parameters())\n",
        "criterion=nn.MSELoss()\n",
        "print(model)\n",
        "\n",
        "total_loss,theta=train_models([model],[optimizer],num_samples=100000,circle_interval=2.0)\n",
        "\n",
        "fig,ax=plt.subplots(figsize=(7,5))\n",
        "ax.plot(total_loss,marker='.',ls='',markersize=.8)\n",
        "ax.set_ylim(0,);ax.set_xlim(0,);ax.grid(alpha=0.2);\n",
        "ax.set_xlabel('training examples');ax.set_ylabel('mean squared loss');\n",
        "\n",
        "output=plot_circles([model],offset=0,CI=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qajejnfa98Zh"
      },
      "source": [
        "## Parallel approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AABjjzF098Zh"
      },
      "source": [
        "One simple approach to apply the method described by Yarin Gal, is to take multiple of the same models and train them independently. This allows each model to take independent paths through parameter space, usually finding their way near some optimal minima. In practice, this allows you to hedge the risk of getting stuck in some local minima and missing out on the global one, if it exists. \n",
        "\n",
        "A visual way of understanding the situation of training a machine learning model, in general, is by considering a 3D surface plot where the x and y dimensions are two parameters you may modify, with the loss on the z axis, or height, which your aim is to minimize. \n",
        "\n",
        "![landscape of deep learning models](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/graphical-idea-backprop.png?raw=1)\n",
        "\n",
        "The surface that the data carves out in this space is predicated by the data; the aim of the model design is then to build a model flexible and robust enough to find the global minima, but not overly complex enough to overfit and get stuck at a local minima. Also, if your model is too simple, it can skip right over all the minima altogether, and not learn the nuance of the process described by the data. Having too high/low of a learning rate can also make the training process difficult."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsWWfgQY98Zi"
      },
      "source": [
        "---\n",
        "# Image Recognition and Transfer Learning\n",
        "\n",
        "![Convolutional Architecture](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/cnn_architecture.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8r6k9iz98Zi"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "Transfer learning is one of the most useful discoveries to come out of the computer vision community. Stated simply, transfer learning allows one model that was trained on different types of images, e.g. dogs vs cats, to be used for a different set of images, e.g. planes vs trains, while reducing the training time dramatically. When Google released ImageNet, they stated it took them over 14 **days** to train the model on some of the most powerful GPUs available at the time. Now, with transfer learning, we will train an, albeit smaller, model in less than 5 minutes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF9D_vi898Zi"
      },
      "source": [
        "The philosophy behind transfer learning is simple. We keep the \"base\" layers of a model frozen since the weights have already been tuned to identify patterns such as lines, circles, and other shapes, and insert layers at the end that will be tuned for the specific task at hand.\n",
        "\n",
        "![](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/NN_vision.png?raw=1)\n",
        "\n",
        "![](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/transfer_learning.png?raw=1)\n",
        "\n",
        "\n",
        "For our task, let's take a look at the King and Queen of the Miami food scene: \n",
        "\n",
        "**The Cuban Sandwich**  \n",
        "\n",
        "![](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/cubano.jpg?raw=1)\n",
        "\n",
        "**The Stuffed Arepa**\n",
        "\n",
        "![](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/arepa.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-29T13:46:28.719471Z",
          "start_time": "2019-04-29T13:46:28.704878Z"
        },
        "id": "HHazrBAH98Zj"
      },
      "source": [
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        " \n",
        "data_transforms = {\n",
        "    'train':\n",
        "        transforms.Compose([\n",
        "            transforms.Resize((224,224)),\n",
        "            transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor()]),\n",
        "    'validation':\n",
        "        transforms.Compose([\n",
        "            transforms.Resize((224,224)),\n",
        "            transforms.ToTensor()])}\n",
        " \n",
        "image_datasets = {\n",
        "    'train':\n",
        "        datasets.ImageFolder('data/cva_data/train', data_transforms['train']),\n",
        "    'validation':\n",
        "        datasets.ImageFolder('data/cva_data/validation', data_transforms['validation'])}\n",
        " \n",
        "dataloaders = {\n",
        "    'train':\n",
        "        torch.utils.data.DataLoader(\n",
        "            image_datasets['train'],\n",
        "            batch_size=32,\n",
        "            shuffle=True,\n",
        "            num_workers=4),\n",
        "    'validation':\n",
        "        torch.utils.data.DataLoader(\n",
        "            image_datasets['validation'],\n",
        "            batch_size=32,\n",
        "            shuffle=False,\n",
        "            num_workers=4)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T01:28:02.580919Z",
          "start_time": "2019-05-02T01:28:01.614605Z"
        },
        "id": "aYDftGNq98Zk"
      },
      "source": [
        "model = models.resnet50(pretrained=True).to(device)\n",
        " \n",
        "# freeze the weights\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "# modify the final layer of resnet50 (called 'fc')\n",
        "# originally model.fc = nn.Linear(2048, 1000) for the 1000 image classes\n",
        "# we modify it to our specific needs\n",
        "\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(2048, 128),\n",
        "    # inplace=True is a shortcut to not modify the forward method \n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Linear(128, 2)).to(device)\n",
        " \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.fc.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IikbBD698Zl"
      },
      "source": [
        "# Get estimate of model size: number of parameters, storage, etc.\n",
        "\n",
        "# input size is based off of the dimensions of a single example in our dataset.\n",
        "summary(model, input_size=(3, 224, 224))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uvz_TM4_98Zm"
      },
      "source": [
        "def train_model(model, criterion, optimizer, num_epochs=3):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        " \n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        " \n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        " \n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        " \n",
        "                if phase == 'train':\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        " \n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        " \n",
        "            epoch_loss = running_loss / len(image_datasets[phase])\n",
        "            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
        " \n",
        "            print(f'{phase} loss: {epoch_loss}, acc: {epoch_acc}')\n",
        "    return model\n",
        " \n",
        "model_trained = train_model(model, criterion, optimizer, num_epochs=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG3txGvM98Zn"
      },
      "source": [
        "### Saving Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZStqUrY998Zo"
      },
      "source": [
        "torch.save(model_trained.state_dict(),'models/cva_weights_new.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbmTmF8m98Zq"
      },
      "source": [
        "### Loading Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-29T13:47:11.945605Z",
          "start_time": "2019-04-29T13:47:11.110004Z"
        },
        "id": "2xhvK0Zj98Zq"
      },
      "source": [
        "model = models.resnet50(pretrained=False).to(device)\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(2048, 128),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Linear(128, 2)).to(device)\n",
        "model.load_state_dict(torch.load('models/cva_weights.h5'))\n",
        "model.eval() # stop training model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-W0-scw98Zr"
      },
      "source": [
        "### Make predictions on test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-29T13:47:15.486794Z",
          "start_time": "2019-04-29T13:47:15.441762Z"
        },
        "id": "Yg5NP4fr98Zr"
      },
      "source": [
        "validation_img_paths = [\"data/cva_data/validation/arepas/00000165.jpg\",\n",
        "                        \"data/cva_data/validation/cubanos/00000037.jpg\",\n",
        "                        \"data/cva_data/validation/cubanos/00000061.jpg\",\n",
        "                        \"data/cva_data/validation/arepas/00000003.jpeg\"]\n",
        "img_list = [Image.open(img_path) for img_path in validation_img_paths]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-29T13:54:38.976494Z",
          "start_time": "2019-04-29T13:54:37.385606Z"
        },
        "id": "mGkOZaCd98Zs"
      },
      "source": [
        "validation_batch = torch.stack([data_transforms['validation'](img).to(device)\n",
        "                                for img in img_list])\n",
        " \n",
        "pred_logits_tensor = model(validation_batch)\n",
        "pred_probs = F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-29T13:54:40.651430Z",
          "start_time": "2019-04-29T13:54:40.088288Z"
        },
        "id": "nyEbGi4N98Zt"
      },
      "source": [
        "fig, axs = plt.subplots(1, len(img_list), figsize=(20, 5))\n",
        "for i, img in enumerate(img_list):\n",
        "    ax = axs[i]\n",
        "    ax.axis('off')\n",
        "    ax.set_title(\"{:.0f}% Arepa, {:.0f}% Cubano\".format(100*pred_probs[i,0],\n",
        "                                                          100*pred_probs[i,1]))\n",
        "    ax.imshow(img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3f9DSrp98Zt"
      },
      "source": [
        "# CNN from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFfDZV5a98Zu"
      },
      "source": [
        "###  `Datasets` and `Dataloaders`\n",
        "\n",
        "In PyTorch, you'll usually create or import a `Dataset` subclass to represent your data. Once you've done that, you can use it to instantiate a `Dataloader` object which allows you to easily iterate over your training set in `BATCH_SIZE` chunks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T15:23:01.483803Z",
          "start_time": "2019-05-01T15:22:52.888024Z"
        },
        "id": "rDY2WCgj98Zu"
      },
      "source": [
        "image_size = 28\n",
        "num_classes = 10\n",
        "num_channels = 1\n",
        "batch_size = 64\n",
        "id_to_label = {\n",
        "    0 :'T-shirt/top',\n",
        "    1 :'Trouser',\n",
        "    2 :'Pullover',\n",
        "    3 :'Dress',\n",
        "    4 :'Coat',\n",
        "    5 :'Sandal',\n",
        "    6 :'Shirt',\n",
        "    7 :'Sneaker',\n",
        "    8 :'Bag',\n",
        "    9 :'Ankle boot'}\n",
        "\n",
        "\n",
        "class FashionDataset(Dataset):\n",
        "\n",
        "    def __init__(self, path, \n",
        "                 image_size, num_channels, image_transform=None):\n",
        "        self.num_channels = num_channels\n",
        "        self.image_size = image_size\n",
        "        self.image_transform = image_transform\n",
        "        data_df = pd.read_csv(path)\n",
        "        self.X = data_df.values[:, 1:]\n",
        "        self.X = self.X.reshape(-1, image_size, image_size, num_channels)\n",
        "        self.X = self.X.astype('float32')\n",
        "        self.y = data_df.values[:, 0]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        batch_X, batch_y = self.X[index], self.y[index]\n",
        "        if self.image_transform is not None:\n",
        "            batch_X = self.image_transform(batch_X)\n",
        "        return batch_X, batch_y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "# This simple transform coverts the image from an numpy array\n",
        "# to a PyTorch tensor and remaps its values from 0-255 to 0-1. \n",
        "# Many other types of transformations are available, and they \n",
        "# can easily be composed into a pipeline. For more info see: \n",
        "# https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "image_transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = FashionDataset(\n",
        "    'fashionmnist/fashion-mnist_train.csv', \n",
        "    image_size, \n",
        "    num_channels, \n",
        "    image_transform)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True)\n",
        "\n",
        "val_dataset = FashionDataset(\n",
        "    'fashionmnist/fashion-mnist_test.csv', \n",
        "    image_size, \n",
        "    num_channels, \n",
        "    image_transform)\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87tWdYgJ98Zv"
      },
      "source": [
        "### Show some examples from the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T01:58:53.841592Z",
          "start_time": "2019-05-02T01:58:53.065507Z"
        },
        "id": "G3_1m7hh98Zw"
      },
      "source": [
        "def plot_images(data, labels, image_size):\n",
        "    fig, axes = plt.subplots(\n",
        "        1, data.shape[0], figsize=(16, 4), \n",
        "        subplot_kw={'xticks':[], 'yticks':[]},\n",
        "        gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(data[i].reshape(image_size, image_size), cmap='binary')\n",
        "        ax.set_xlabel(labels[i])\n",
        "        \n",
        "images = train_dataset.X[:10]\n",
        "class_ids = train_dataset.y[:10]\n",
        "class_labels = [id_to_label[class_id] for class_id in class_ids]\n",
        "plot_images(images, class_labels, image_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiTMWwoN98Zy"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHn8Z9og98Zy"
      },
      "source": [
        "This baseline network uses a single convolution with only 4 filters. Because of the simplicity of our dataset, it still manages to achieve nearly 90% accuracy after only 5 epochs.\n",
        "\n",
        "Experiment and see if you can increase the accuracy on the validation set to above 95%.\n",
        "\n",
        "Things you might try:\n",
        "\n",
        "* Increasing the number of filters per convolution.\n",
        "* Adding more convolutions.\n",
        "* Adding a `BatchNorm2d` layer after `Conv2d`.\n",
        "* Increasing the number of epochs.\n",
        "* Changing the kernel size.\n",
        "* Using different types of pooling or using stride > 1 in convolutional layers instead of pooling.\n",
        "\n",
        "You might also find the [PyTorch API reference](https://pytorch.org/docs/stable/nn.html) useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrKe0kZU98Zy"
      },
      "source": [
        "### Determining Output Size after Convolution and MaxPool\n",
        "\n",
        "When dealing with any neural net model, we have to ensure that each one of our layers is receiving the correct input size from its previous layers. This is easier said than done as, in Computer Vision, we have to convert from 2D layers such as convolutions, batch normalizations, and max pooling layers to communicate properly with a linear layer that follows. To do this, we have to ensure that our calculations for the **input** to each layer is correct when we instantiate our model.\n",
        "\n",
        "Linear layers are simple, the output size is stated when we instantiate. The difficulty lies in Conv2D layers since PyTorch calculates the output size using a formula and we need to determine what that size is by hand if we need it to pass it as input for other layers.\n",
        "\n",
        "To determine the output size after 2D layers we usually have to do it separately for our height and width, but since we are dealing with square images they will give us the same result. Furthermore, we'll remove any values that are set to zero in our model to reduce the complexity of the equation. For more information on output size for different functions, refer to the PyTorch API.\n",
        "\n",
        "#### Conv2D\n",
        "\n",
        "$$H_{out} = \\frac{H_{in} - kernel + 2 * padding}{stride} + 1$$\n",
        "\n",
        "\n",
        "#### MaxPool2D\n",
        "$$H_{out} = \\frac{H_{in} - kernel + 2*padding}{stride} + 1$$\n",
        "\n",
        "\n",
        "\n",
        "Let's use the numbers from our `FashionModel` (see below) to determine what the outputs are after our first combo of `Conv2d` and `MaxPool2d`\n",
        "\n",
        "Since our images are square, (28x28) we only need to do this calcluation once.\n",
        "\n",
        "**Size of image:** 28\n",
        "\n",
        "**Output size after Conv2d:**\n",
        "$$H_{out} = \\frac{28 - 3 + 2*1}{1} + 1 = 28$$\n",
        "\n",
        "**Output size after MaxPool2d:**\n",
        "$$H_{out} = \\frac{28 - 2 + 2*0}{2} + 1 = 14$$\n",
        "\n",
        "\n",
        "If passing to a linear layer we need to multiply `output_channels` from our `Conv2d` times the `output_size_height` and `output_size_width` (in our case they are the same) to properly rescale our tensor. \n",
        "\n",
        "**Output channels (after Conv2d):** 16\n",
        "\n",
        "**MaxPool Output Size:** 14 (same for both height and width)\n",
        "\n",
        "Thus, our linear layer needs and input of size `14 * 14 * 16`\n",
        "\n",
        "For more info see [here.](https://www.quora.com/How-can-I-calculate-the-size-of-output-of-convolutional-layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T02:14:36.037653Z",
          "start_time": "2019-05-02T02:14:36.019257Z"
        },
        "id": "ksWzfB3m98Zy"
      },
      "source": [
        "class FashionModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.conv_1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
        "        # Add more conv2d layers\n",
        "        # recall that the input channel size for the new layer is 16! \n",
        "        # Make the outputs 32 and 64 to create 3 total convolution layers\n",
        "        \n",
        "        \n",
        "        # ------------- CODE GOES HERE ------------- #\n",
        "        \n",
        "        \n",
        "        # to determine the correct inputs for the last linear layer you'll need the formulas: \n",
        "        # output_size of conv2d = (input_width - kernel_size + 2*padding)/stride + 1\n",
        "        # output_size of maxpool = (output_size of conv2d - kernel_size + 2*padding)/stride + 1\n",
        "        \n",
        "        # See cell above for more detail\n",
        "        \n",
        "        # If adding an extra conv layers, the input into nn.Linear needs to change!\n",
        "        self.linear = nn.Linear(14 * 14 * 16, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv_1(x)\n",
        "        # Uncomment as more convolution layers are added\n",
        "        # x = self.conv_2(x)\n",
        "        # x = self.conv_3(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Instantiate the model.    \n",
        "model = FashionModel(num_channels, num_classes)\n",
        "\n",
        "# Send the model's tensors to the GPU (if available).\n",
        "model = model.to(device)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMphQPq_98Zz"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T02:14:51.121614Z",
          "start_time": "2019-05-02T02:14:42.815674Z"
        },
        "id": "v63uptok98Z0"
      },
      "source": [
        "num_epochs = 10\n",
        "log_freq = 100\n",
        "checkpoint_path = 'checkpoint.pickle'\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()  # Switch to training mode.\n",
        "    print(f'Starting epoch {epoch}.')\n",
        "    epoch_start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "    for batch_id, (batch_X, batch_y) in enumerate(train_dataloader):\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        output = model(batch_X)\n",
        "        loss = criterion(output, batch_y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Periodically print the loss and prediction accuracy.\n",
        "        running_loss += loss.item()\n",
        "        y_pred = output.argmax(dim=-1)\n",
        "        running_accuracy += accuracy_score(batch_y.cpu(), y_pred.cpu())\n",
        "        if batch_id % log_freq == log_freq - 1:\n",
        "            average_loss = running_loss / log_freq\n",
        "            average_accuracy = running_accuracy / log_freq\n",
        "            print(f'Mini-batch: {batch_id + 1}/{len(train_dataloader)} '\n",
        "                  f'Loss: {average_loss:.5f} Accuracy: {average_accuracy:.5f}')\n",
        "            running_loss = 0.0\n",
        "            running_accuracy = 0.0\n",
        "            \n",
        "    # Log elapsed_time for the epoch.            \n",
        "    elapsed_time = time.time() - epoch_start_time\n",
        "    print(f'\\nEpoch {epoch} completed in {elapsed_time // 60:.0f} minutes '\n",
        "          f'{elapsed_time % 60:.0f} seconds.')\n",
        "    \n",
        "    # Calculate and log loss on validation set.\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        running_accuracy = 0.0\n",
        "        for batch_id, (batch_X, batch_y) in enumerate(val_dataloader):\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            output = model(batch_X)\n",
        "            loss = criterion(output, batch_y)\n",
        "            running_loss += loss.item()\n",
        "            y_pred = output.argmax(dim=-1)  \n",
        "            running_accuracy += accuracy_score(batch_y.cpu(), y_pred.cpu())\n",
        "        average_loss = running_loss / len(val_dataloader)\n",
        "        average_accuracy = running_accuracy / len(val_dataloader)\n",
        "        print(f'Val Loss: {average_loss:.5f} Val Accuracy: {average_accuracy:.5f}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAuXhyQb98Z6"
      },
      "source": [
        "# Semi-Supervised Learning - Style Transfer\n",
        "\n",
        "Style transfer, to me, is one of the coolest \"discoveries\" in the computer vision and deep learning community from the past few years. In essence, it allows us to take the \"content\" from an image (shapes, objects, arrangements) and reproduce a new target that is in the \"style\" (style, colors, textures) of another. \n",
        "\n",
        "We'll be taking inspiration from the paper, [Image Style Transfer Using Convolutional Neural Networks, by Gatys](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf), and implementing the model in PyTorch.\n",
        "\n",
        "In the paper, style transfer uses the features found in the 19-layer VGG Network, which are comprised of a series of convolutional and pooling layers, and a few fully-connected layers. (Recall that this is similar to many of the computer vision models discussed earlier) \n",
        "\n",
        "In the image below, the convolutional layers are named by stack and their order in the stack.\n",
        "\n",
        "For example, `Conv_1_1` is the first convolutional layer that an image is passed through in the *first* stack. `Conv_2_1` is the first convolutional layer in the *second* stack. The deepest convolutional layer in the network is `Conv_5_4`.\n",
        "\n",
        "<img src='https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/vgg19_convlayers.png?raw=1' width=80% />\n",
        "\n",
        "Style transfer relies on separating the content and style of an image. To do so, we aim to create a new **target** image which should contain our desired content and style components.\n",
        "\n",
        "**Note:**\n",
        "* objects and their arrangement are similar to that of the **content image**\n",
        "* style, colors, and textures are similar to that of the **style image**\n",
        "\n",
        "<img src='https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/style-transfer-example.jpg?raw=1' width=80% />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH93VNUL98Z6"
      },
      "source": [
        "## Load VGG19\n",
        "\n",
        "VGG19 is split into two portions:\n",
        "* `vgg19.features` - contains all the convolutional and pooling layers\n",
        "* `vgg19.classifier` - contains the three fully connected layers layers at the end\n",
        "\n",
        "We only need the `features` portion, which we're going to load in and \"freeze\" the weights of. This is similar to what we did for our transfer learning section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQXtdum498Z7"
      },
      "source": [
        "# get the \"features\" portion of VGG19\n",
        "vgg = models.vgg19(pretrained=True).features\n",
        "\n",
        "# freeze all VGG parameters since we're only optimizing the target image\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad_(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KCR6u8L98Z9"
      },
      "source": [
        "# move the model to GPU (if available)\n",
        "vgg.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVGVbkt_98Z-"
      },
      "source": [
        "### Load in Content and Style Images\n",
        "\n",
        "Load in any images you want! The code below is a helper function for loading in any type and size of image. The `load_image` function also converts images to normalized Tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emPYYCP398Z_"
      },
      "source": [
        "def load_image(img_path, max_size=400, shape=None):\n",
        "    ''' Load in and transform an image, making sure the image\n",
        "       is <= 400 pixels in the x-y dims.'''\n",
        "    if \"http\" in img_path:\n",
        "        response = requests.get(img_path)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    else:\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "    \n",
        "    # large images will slow down processing\n",
        "    if max(image.size) > max_size:\n",
        "        size = max_size\n",
        "    else:\n",
        "        size = max(image.size)\n",
        "    \n",
        "    if shape is not None:\n",
        "        size = shape\n",
        "        \n",
        "    in_transform = transforms.Compose([\n",
        "                        transforms.Resize(size),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
        "                                             (0.229, 0.224, 0.225))])\n",
        "\n",
        "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
        "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
        "    \n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z1U5NhX98Z_"
      },
      "source": [
        "# load in content and style image\n",
        "content = load_image('data/img/Nala.jpg').to(device)\n",
        "# Resize style to match content, makes code easier\n",
        "style = load_image('data/img/style_images/dayofthedead.jpg', shape=content.shape[-2:]).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIzfP7-M98aA"
      },
      "source": [
        "# helper function for un-normalizing an image \n",
        "# and converting it from a Tensor image to a NumPy image for display\n",
        "def im_convert(tensor):\n",
        "    \"\"\" Display a tensor as an image. \"\"\"\n",
        "    \n",
        "    image = tensor.to(\"cpu\").clone().detach()\n",
        "    image = image.numpy().squeeze()\n",
        "    image = image.transpose(1,2,0)\n",
        "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
        "    image = image.clip(0, 1)\n",
        "\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YQn4XdO98aB"
      },
      "source": [
        "# display the images\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "# content and style ims side-by-side\n",
        "ax1.imshow(im_convert(content))\n",
        "ax2.imshow(im_convert(style))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_SLPHnw98aC"
      },
      "source": [
        "## VGG19 Layers\n",
        "\n",
        "To get the content and style representations of an image, we have to pass an image forward throug the VGG19 network until we get to the desired layer(s) and then get the output from that layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjOozSKG98aC"
      },
      "source": [
        "# print out VGG19 model so you can see the names of all the layers\n",
        "print(vgg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOKXQcXC98aE"
      },
      "source": [
        "## Content and Style Features\n",
        "\n",
        "Below, complete the mapping of layer names to the names found in the paper for the _content representation_ and the _style representation_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P8dLNmi98aE"
      },
      "source": [
        "def get_features(image, model, layers=None):\n",
        "    \"\"\" Run an image forward through a model and get the features for \n",
        "        a set of layers. Default layers are for VGGNet matching Gatys et al (2016)\n",
        "    \"\"\"\n",
        "    \n",
        "    ## Here we gather the layers need to preserve style and content of an image\n",
        "    if layers is None:\n",
        "        layers = {'1': 'relu1_1',\n",
        "                  '6': 'relu2_1', \n",
        "                  '11': 'relu3_1', \n",
        "                  '20': 'relu4_1',\n",
        "                  '13': 'relu3_2',  ## content representation\n",
        "                  '29': 'relu5_1'}\n",
        "        \n",
        "    features = {}\n",
        "    x = image\n",
        "    # model._modules is a dictionary holding each module in the model\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "            \n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_9ncp6S98aH"
      },
      "source": [
        "## Gram Matrix \n",
        "\n",
        "The output of every convolutional layer is a Tensor with dimensions associated with the `batch_size`, a depth, `d` and some height and width (`h`, `w`). The Gram matrix of a convolutional layer can be calculated as follows:\n",
        "* Get the depth, height, and width of a tensor using `batch_size, d, h, w = tensor.size`\n",
        "* Reshape that tensor so that the spatial dimensions are flattened\n",
        "* Calculate the gram matrix by multiplying the reshaped tensor by it's transpose "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l1_Lb-X98aH"
      },
      "source": [
        "def gram_matrix(tensor):\n",
        "    \"\"\" Calculate the Gram Matrix of a given tensor \n",
        "        Gram Matrix: https://en.wikipedia.org/wiki/Gramian_matrix\n",
        "    \"\"\"\n",
        "    \n",
        "    # get the batch_size, depth, height, and width of the Tensor\n",
        "    _, d, h, w = tensor.size()\n",
        "    \n",
        "    # reshape so we're multiplying the features for each channel\n",
        "    tensor = tensor.view(d, h * w)\n",
        "    \n",
        "    # calculate the gram matrix\n",
        "    gram = tensor @ tensor.t()\n",
        "    \n",
        "    return gram "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJIQ9riA98aK"
      },
      "source": [
        "## We're Almost there!\n",
        "\n",
        "Here's what our helper functions do:\n",
        "\n",
        "* Extract the features for our content and style images from VGG19\n",
        "* Compute the Gram Matrix for a given convolutional layer\n",
        "\n",
        "\n",
        "What we need:\n",
        "* Put it all together!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b8fo8gg98aK"
      },
      "source": [
        "# get content and style features only once before training\n",
        "content_features = get_features(content, vgg)\n",
        "style_features = get_features(style, vgg)\n",
        "\n",
        "# calculate the gram matrices for each layer of our style representation\n",
        "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
        "\n",
        "# create a third \"target\" image and prep it for change\n",
        "# it is a good idea to start off with the target as a copy of our *content* image\n",
        "# then iteratively change its style\n",
        "target = content.clone().requires_grad_(True).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzb2iaqA98aL"
      },
      "source": [
        "## Loss and Weights\n",
        "\n",
        "#### Individual Layer Style Weights\n",
        "\n",
        "In the script below, we have the option to weight the style representation at each relevant layer. This will allow us to fine tune what effect size we want for each layer - earlier layers have larger style artifacts and later layers place emphasis on smaller features. Remember, each layer is a different size and by combining them we can create multi-scale style representations.\n",
        "\n",
        "The paper suggests using a range between 0-1 to weight the layers.\n",
        "\n",
        "#### Content and Style Weight\n",
        "\n",
        "The paper defines a **style ratio** of $\\alpha/\\beta$, where  $\\alpha$ is the `content_weight` and  $\\beta$ is the `style_weight`. This ratio will affect how _stylized_ the final image is. It's recommended that to leave the content_weight = 1 and set the style_weight to achieve the ratio needed for a desired effect style. Note that this is not exact science, there will be lots of tuning of the ratio and the weights of the layers to get a result we're pleased with.\n",
        "\n",
        "Remember - the reason this is called \"Semi-Supervised\" is because there is no right answer. We decide when to stop the training based on intermediate results we plot and stop when we are happy with what we see."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GXW_90X98aL"
      },
      "source": [
        "# weights for each style layer \n",
        "# weighting earlier layers more will result in *larger* style artifacts\n",
        "# notice we are excluding `conv4_2` our content representation\n",
        "style_weights = {'relu1_1': 1.,\n",
        "                 'relu2_1': 0.75,\n",
        "                 'relu3_1': 0.4,\n",
        "                 'relu4_1': 0.3,\n",
        "                 'relu5_1': 0.2}\n",
        "\n",
        "content_weight = 1  # alpha\n",
        "style_weight = 1e8  # beta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-Aa1VAY98aM"
      },
      "source": [
        "## Update the Target & Calculate Losses\n",
        "\n",
        "Like in every training loop we've seen before, we need to decide on how many passes we want to do on our model (via gradient descent). The difference here is that we will be changing our **target** image and nothing about the VGG19 model or our original content and style images. Since this is semi-supervised, the number of steps to choose is up to you. Keep in mind that after ~50,000 steps you probably won't see any noticeable differences in the images and by using ~2,000 steps you can see early on whether the style ratio ($\\alpha/\\beta$) is giving the desired effect. \n",
        "\n",
        "Experiment with different weights or images to see some really cool effects!\n",
        "\n",
        "#### Content Loss\n",
        "\n",
        "The content loss will be the mean squared difference between the target and content features at layer `conv4_2`. This can be calculated as follows: (see paper)\n",
        "```\n",
        "content_loss = torch.mean(torch.abs(target_features['conv4_2'] - content_features['conv4_2']))\n",
        "```\n",
        "\n",
        "#### Style Loss\n",
        "\n",
        "The style loss is calculated in a similar way, but we have to iterate through the layers specified by name in our dictionary `style_weights`. \n",
        "\n",
        "> Calculate the gram matrix for the target image, `target_gram` and style image `style_gram` at each of these layers and compare those gram matrices, calculating the `layer_style_loss`. \n",
        "\n",
        "#### Total Loss\n",
        "\n",
        "Finally, the total loss is calculated by adding up the individual style and content losses and weighting them with the specified alpha and beta values chosen.\n",
        "\n",
        "Intermittently, we'll print out an intermediate image and its loss - don't be alarmed if the loss is very large! It takes some time for an image's style to change and you should focus on the appearance of your target image rather than any loss value, but we should still be seeing the loss go down over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tydP-K9c98aM"
      },
      "source": [
        "# for displaying the target image, intermittently\n",
        "show_every = 400\n",
        "\n",
        "# iteration hyperparameters\n",
        "optimizer = torch.optim.Adam([target], lr=0.003)\n",
        "steps = 2000  # decide how many iterations to update your image\n",
        "\n",
        "for step in range(1, steps+1):\n",
        "    \n",
        "    # get the features from your target image\n",
        "    target_features = get_features(target, vgg)\n",
        "    \n",
        "    # the content loss\n",
        "    content_loss = torch.mean(torch.abs(target_features['relu3_2'] - content_features['relu3_2']))\n",
        "    \n",
        "    # the style loss\n",
        "    # initialize the style loss to 0\n",
        "    style_loss = 0\n",
        "    # then add to it for each layer's gram matrix loss\n",
        "    for layer in style_weights:\n",
        "        # get the \"target\" style representation for the layer\n",
        "        target_feature = target_features[layer]\n",
        "        target_gram = gram_matrix(target_feature)\n",
        "        _, d, h, w = target_feature.shape\n",
        "        # get the \"style\" style representation\n",
        "        style_gram = style_grams[layer]\n",
        "        # the style loss for one layer, weighted appropriately\n",
        "        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
        "        # add to the style loss\n",
        "        style_loss += layer_style_loss / (d * h * w)\n",
        "        \n",
        "    # calculate the *total* loss\n",
        "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
        "    \n",
        "    # update your target image\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # display intermediate images and print the loss\n",
        "    if  step % show_every == 0:\n",
        "        print('Total loss: ', total_loss.item())\n",
        "        plt.imshow(im_convert(target))\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US9otQ0e98aN"
      },
      "source": [
        "## Display the Target Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVB8O5_998aO"
      },
      "source": [
        "# display content and target image\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "ax1.imshow(im_convert(content))\n",
        "ax2.imshow(im_convert(target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9IvOCk598aP"
      },
      "source": [
        "# display style and target image\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "ax1.imshow(im_convert(style))\n",
        "ax2.imshow(im_convert(target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo6F3GOJrISA"
      },
      "source": [
        "plt.imshow(im_convert(target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdzRCNyauoww"
      },
      "source": [
        "fig, ax = plt.subplots(1,1,figsize=(20,20))\n",
        "ax.imshow(im_convert(target))\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['left'].set_visible(False)\n",
        "ax.spines['bottom'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "plt.savefig(\"nala_mod.png\",bbox_inches=\"tight\",pad_inches=0.02,dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_ZqoJnWyDEz"
      },
      "source": [
        "# display fully stylized image vs style\n",
        "file_path = '/content/data/img/nala_mod.png'\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 10))\n",
        "ax1.imshow(im_convert(content))\n",
        "ax2.imshow(im_convert(style))\n",
        "ax3.imshow(Image.open(file_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UZVR5_V98aR"
      },
      "source": [
        "---\n",
        "# RNN Character Level Generation\n",
        "\n",
        "Text generation is a fun way to familiarize yourself with Recurrent Neural Nets.\n",
        "\n",
        "In this notebook, we will deal with **character-level** text generation and why they can be just as useful as word-level text generation.\n",
        "\n",
        "For this example, we'll be using text files to generate code similar to our input. In other words, if we put in Trump tweets, our generator should output words that sound like Trump.\n",
        "\n",
        "### Our current understanding of RNNs\n",
        "![](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/LSTM_next_character.png?raw=1)\n",
        "\n",
        "\n",
        "### Reminder\n",
        "\n",
        "![](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/rnn_unrolling.png?raw=1)\n",
        "\n",
        "#### \"RNNs have a hidden state that feeds back into the cell at the next time step\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_GV03CE98aR"
      },
      "source": [
        "## What is actually going on?\n",
        "\n",
        "### Example with sequence length 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W1SYysW98aR"
      },
      "source": [
        "![](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/rnn_forward.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s_MsXJM98aR"
      },
      "source": [
        "### What about the backwards pass?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3fvQzdk98aS"
      },
      "source": [
        "![](https://github.com/robert-alvarez/pytorch_tutorial/blob/master/data/img/rnn_backward.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AXEdHUq98aS"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9licgHnh98aS"
      },
      "source": [
        "# install unidecode\n",
        "!pip install unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-30T21:57:47.121064Z",
          "start_time": "2019-04-30T21:57:47.113879Z"
        },
        "id": "EwM19pvC98aU"
      },
      "source": [
        "# Read in text and change unicode characters to ASCII\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-30T21:57:48.885409Z",
          "start_time": "2019-04-30T21:57:48.873437Z"
        },
        "id": "5nkrsGUh98aV"
      },
      "source": [
        "# read in file to train RNN\n",
        "file = unidecode.unidecode(open('data/shakespeare.txt').read())\n",
        "file_len = len(file)\n",
        "print(f'file_len = {file_len}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReHlckvd98aW"
      },
      "source": [
        "To give our model inputs from this large string of text, we'll split it up into chunks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-30T21:57:52.871182Z",
          "start_time": "2019-04-30T21:57:52.861252Z"
        },
        "id": "lKIIRWCE98aW"
      },
      "source": [
        "chunk_len = 400\n",
        "\n",
        "def random_chunk():\n",
        "    start_index = random.randint(0, file_len - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file[start_index:end_index]\n",
        "\n",
        "print(random_chunk())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi0rBFSe98aW"
      },
      "source": [
        "## Build Model\n",
        "\n",
        "This model will take as input the character for step $t$ and is expected to output the next character for step $t+1$. There are three layers - one linear layer that encodes the input character into an internal state, one GRU layer (which may itself have multiple layers) that operates on that internal state and a hidden state, and a decoder layer that outputs the probability distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-30T21:58:21.474477Z",
          "start_time": "2019-04-30T21:58:21.464818Z"
        },
        "id": "GDk8bHgi98aX"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        output = self.encoder(input.view(1, -1))\n",
        "        output, hidden = self.gru(output.view(1, 1, -1), hidden)\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        \n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        return Variable(torch.randn(self.n_layers, 1, self.hidden_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXw2G1Fu98aZ"
      },
      "source": [
        "## Inputs and Targets\n",
        "\n",
        "Now that we've defined our model, we need to give it both input data, via our chunks, and our target data. Each character is one-hot encoded to the vocab size "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-30T21:58:24.169426Z",
          "start_time": "2019-04-30T21:58:24.161142Z"
        },
        "id": "JYBv5jbj98aZ"
      },
      "source": [
        "def char2tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for char in range(len(string)):\n",
        "        if string[char] in all_characters:\n",
        "            tensor[char] = all_characters.index(string[char])\n",
        "        else:\n",
        "            tensor[char] = 94 #predict space if character unknown\n",
        "        \n",
        "    return Variable(tensor)\n",
        "\n",
        "# Let's see it in action.\n",
        "print(char2tensor('Podium0123abczABC'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQeU8ig398aa"
      },
      "source": [
        "Now that we can generate chunks of data, we can build our inputs and targets.\n",
        "\n",
        "Our inputs will be all of the chunk except for the last letter. \n",
        "\n",
        "Our target will be all of the chunk except for the first letter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-30T21:58:26.734169Z",
          "start_time": "2019-04-30T21:58:26.721769Z"
        },
        "id": "-3WJANx798aa"
      },
      "source": [
        "def random_training_set():\n",
        "    chunk = random_chunk()\n",
        "    inp = char2tensor(chunk[:-1])\n",
        "    target = char2tensor(chunk[1:])\n",
        "    return inp, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgVLzwWA98aa"
      },
      "source": [
        "## Evaluating the Model\n",
        "\n",
        "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-30T21:58:29.035941Z",
          "start_time": "2019-04-30T21:58:28.959259Z"
        },
        "id": "rbdP6olM98ab"
      },
      "source": [
        "def evaluate(model, prime_str='A', predict_len=100, temperature=0.8):\n",
        "    \n",
        "    hidden = model.init_hidden()\n",
        "    prime_input = char2tensor(prime_str)\n",
        "    predicted = prime_str\n",
        "    \n",
        "    # use priming string to build up hidden state\n",
        "    \n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = model(prime_input[p], hidden)\n",
        "    inp = prime_input[-1]\n",
        "    \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = model(inp, hidden)\n",
        "        \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        \n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = all_characters[top_i]\n",
        "        predicted += predicted_char\n",
        "        inp = char2tensor(predicted_char)\n",
        "        \n",
        "    return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCXSgyAq98ac"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-30T21:58:32.785485Z",
          "start_time": "2019-04-30T21:58:32.776492Z"
        },
        "id": "RFvGhOSG98ac"
      },
      "source": [
        "# helper function\n",
        "import time, math\n",
        "\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s/60)\n",
        "    s -= m*60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-30T21:58:35.268443Z",
          "start_time": "2019-04-30T21:58:35.251463Z"
        },
        "id": "S4VVSZk598ac"
      },
      "source": [
        "# The actual training part\n",
        "def train(inp, target):\n",
        "    hidden = model.init_hidden()\n",
        "    model.zero_grad()\n",
        "    loss = 0\n",
        "\n",
        "    for char in range(chunk_len):\n",
        "        output, hidden = model(inp[char], hidden)\n",
        "        loss += criterion(output, target[char].unsqueeze(0))\n",
        "\n",
        "    loss.backward()\n",
        "    model_optimizer.step()\n",
        "\n",
        "    return loss.data.item() / chunk_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZGTd4Ta98ad"
      },
      "source": [
        "# parameters\n",
        "n_epochs = 1000\n",
        "print_every = 100\n",
        "plot_every = 10\n",
        "hidden_size = 256\n",
        "n_layers = 2\n",
        "learning_rate = 0.001\n",
        "\n",
        "# model declaration\n",
        "model = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "model_optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    loss = train(*random_training_set())       \n",
        "    loss_avg += loss\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
        "        print(evaluate(model, 'A ', 100), '\\n')\n",
        "\n",
        "    if epoch % plot_every == 0:\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        loss_avg = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH0W9ZUD98ae"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUFswDvk98af"
      },
      "source": [
        "### Load pre-trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmgQhWKUVJ8o"
      },
      "source": [
        "# model declaration\n",
        "hidden_size = 256\n",
        "billy = RNN(n_characters, hidden_size, n_characters, n_layers=2)\n",
        "billy.load_state_dict(torch.load('data/models/shakespeare_weights.h5'))\n",
        "cells = RNN(n_characters, hidden_size, n_characters, n_layers=2)\n",
        "cells.load_state_dict(torch.load('data/models/cell_weights.h5'))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T12:59:59.874947Z",
          "start_time": "2019-05-01T12:59:59.537376Z"
        },
        "id": "jGtRIvOi98ah"
      },
      "source": [
        "# Evaluate Billy Shakespeare\n",
        "print(evaluate(billy, 'To be or not to be: ', predict_len=200, temperature=0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-01T18:12:56.428857Z",
          "start_time": "2019-05-01T18:12:56.112249Z"
        },
        "id": "ZKYO0B8A98ai"
      },
      "source": [
        "# Evaluate NLP model of math biology latex file\n",
        "print(evaluate(cells, 'The equation ', predict_len=200, temperature=0.4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzJ43EHb98aj"
      },
      "source": [
        "# Resources\n",
        "\n",
        "[tensor images](https://hackernoon.com/learning-ai-if-you-suck-at-math-p4-tensors-illustrated-with-cats-27f0002c9b32)\n",
        "\n",
        "[alien vs predator](https://deepsense.ai/keras-vs-pytorch-avp-transfer-learning/)\n",
        "\n",
        "[unreasonable reffectiveness of neural nets](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "\n",
        "[pytorch.org](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
        "\n",
        "[Richard Galvez](https://richardagalvez.github.io/)\n",
        "\n",
        "[Visualizing and Understanding Convolutional Networks](https://arxiv.org/pdf/1311.2901.pdf)\n",
        "\n",
        "[Image Style Transfer](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)\n",
        "\n",
        "[Transfer Learning Image](https://www.researchgate.net/figure/Illustration-of-transfer-learning-concept-where-the-rst-layers-in-network-A-and-network-B_fig2_316748306)\n",
        "\n",
        "[Udacity](https://github.com/udacity/deep-learning-v2-pytorch)\n"
      ]
    }
  ]
}